\section{Benchmarks}
\label{datasets}
Our benchmarks were derived from Wikidata.  Specifically, we used the \textit{also known as} property from Wikidata to get alternate forms for the same entity name for people as well as companies.  For company names, we augmented the names and surface forms in Wikidata with data from the SEC \footnote{\url{https://www.sec.gov/dera/data/financial-statement-and-notes-data-set.html}}, which has former and more recent names for companies.  There were 213106 names for people from the specific dump we extracted, and 70946 names for companies.  The extracted files and the cleansing code are available on our repository. The extracted files however contained noise that we cleaned up programmatically.  We describe the clean up rules for people and for companies separately because they were somewhat different.  In the case of people's names, we also augmented the data so the system could learn some common rules that define variants of a person's name.  This was not possible for company names.

\subsection{Cleansing people's names}
Wikidata has a number of historical figures which are not really names of people (e.g. Queen Elizabeth, Pope Leo).  If we detected a title in the name referring to royalty or qualifiers or Roman numerals which strongly indicated royalty, we dropped the name.  We also removed punctuations such as '...', and anything that was placed in parenthesese because they were irrelevant to the name (e.g. a qualification such as the son of Jacob might appear in parentheses after a name).  Although we got the extract for English, there were frequently names in Chinese, Korean, Cyrillic, etc.  We removed these and restricted ourselves to names in ISO-8859-1 unicode.  All punctuations such as ',', '-', '.' etc were retained for names because they are strong indicators of how a name needs to be parsed.  People in wikidata have the main name for the entity, with aliases for the person specified in a different property.  We made sure that every alternate form had at least one name part in common with the main name to rule out 'nicknames' (e.g. `Father of the Nation' for George Washington).  We also dropped cases where the last name of a person was different (usually because a woman's name changed after marriage).      

\subsection{Cleansing company names}
As with people's names, we removed any text in parentheses because it usually was a qualification (e.g., IBM (company)).  We restricted ourselves to unicode ISO-8859-1.  The SEC data had a lot of strange company names that could be described with the regex pattern T[0-9]+ or [0-9]+, and we dropped these.  We tried to ensure we included names that shared some subset of characters with the main name, ensuring we would not drop acronyms when possible.  The check for acronyms tested if any of the initial letters of each name part occurred in the name.

\subsection{Augmenting people's names}
In many cases, we had no alternate forms for a person's name even if we did have their main name.  We augmented the data with the following rules.  If the main name for the person in Wikidata had two parts, we created new source forms as follows: 
\begin{itemize}
\item Last Name, First Name
\item First Name Initial . Last Name
\item Last Name , First Name Initial
\end{itemize}
If the main name in Wikidata had three parts, we created these additional source forms in addition to the ones listed above:
\begin{itemize}
\item First Name Middle Name Initial Last Name
\item Last Name, First Name Middle Name
\item Last Name, First Name Middle Initial .
\end{itemize}
   
The data was then split with a 60-20-20 ratio to provide training, validation and test data respectively.  Each run was conducted with a different random split to ensure generalization of results.
