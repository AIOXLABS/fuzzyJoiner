\section{Related Work}
Extensions in data management systems for handling joins across different spellings for instance, typically use string similarity algorithms such as edit-distance, Jaro-Winkler and TF-IDF (e.g., \cite{Cohen2003}).  String matching algorithms often do not work for merging different forms of the same entity because valid transformations of entity names can yield very different strings (e.g., \textit{John Smith} is more similar to \textit{John Adams} than \textit{Adams, John}, but it is \textit{Adams, John} that is a valid alternate form).

More recently, data driven approaches have emerged as a powerful alternative to string matching techniques for merging data.  Data driven approaches mine patterns in the data to determine the `rules' for joining a given entity type.  One example of such an approach is illustrated in \cite{He:2015:SJS:2824032.2824036}, which determines which cell values should be joined based on whether those cell values co-occur on the same row across disparate tables in a very large corpus of data.  Another example of a data driven approach is work by \cite{auto-join-joining-tables-leveraging-transformations} where program synthesis techniques are used to learn the right set of transformations needed to perform the entity matching operation.  Our approach for merging datasets is much more general than either approach because the mapping function generalizes the set of transformations that are allowed across surface forms of an entity even when the explicit rules for identifying two surface forms as the same are not explicitly expressible as program transforms.

The idea of building joint embeddings for merging followed by nearest neighbors search has been applied recently to a slightly different problem of linking relational tuple embeddings with embeddings of other relational tuples or unstructured text \cite{Bordawekar18}, \cite{IDEL18}).  For the problem of linking tuples, each model that is learnt is specific to the database it was trained on.  Our contribution is that we developed general purpose embedding models for merging alternate surface forms of key entities such as people and companies leveraged from linked open data.  Once such models are built, they can be applied to joining any two datasets that share that semantic type.

The use of nearest neighbors algorithms for triplet mining has been applied by \cite{DBLP:journals/corr/KumarHC0D17}, but their focus is on mining semi-hard negatives, that is focusing on negatives where at least one positive is closer to the anchor.  In general, this means that one cannot look at fixed neighborhood sizes in building triplets.  In other words, if all the positives are further away from the anchor than the negatives in a given neighborhood size of $k$, it means that $k$ needs to be expanded until a neighborhood size is found that has the right characteristics (i.e., at least one positive that is closer than the negattives which can be chosen).   Our approach much more efficient because it relies on a fixed $k$ to generate samples.
TBD - related work on triplet selection and losses.
