\subsubsection{Triplet Selection}
Deep metric learning for faces is a difficult problem for neural networks to learn because improper sample selection can make it difficult for the network to converge.  In Schroff et al.'s work~\cite{DBLP:conf/cvpr/SchroffKP15} they discuss various online and offline approaches for the problem of triplet selection.  Conceptually, sampling the `right' triples for fast network learning, as Schroff et al. point out requires sampling a set of \textit{hard positives} and a set of \textit{hard negatives}, where a hard positive is defined as $argmax^i \| x_{a} - x_{p}^i \|^2$, where $i$ ranges over all $x_p$ and a hard negative is defined as $argmin^i \| x_{a} - x{_n}^i \|^2$.  

However, it is infeasible to compute these values for the entire dataset for selection.  As a result, the triplets can be generated either offline, for every n steps, or online, by selecting the hard positive and negative exemplars within a batch of examples.  In Schroff et al.'s work, they focus on the online generation case.  In practice, instead of focusing on finding \textit{hard positives}, they instead pair every possible positive in the sample with selected negatives, since the set of positives is usually quite small.  Furthermore, for negative examples, they try to select what they term \textit{semi-hard negatives} instead of \textit{hard negatives}, where a \textit{semi-hard negative} has the property that $\|x_a - x_p \|^2 < \|x_a - x_n \|^2$.  That is, \textit{semi-hard negative} is further from the \textit{anchor} than the \textit{positive}, but is not distant by the margin $\alpha$ that is used in the triplet-loss function.  Schroff et al. argue selecting \textit{hard negatives} may in practice lead to bad local minima early on in training, and lead to a collapsed model where the function always learns to return a value of 0, regardless of the pair being considered.
