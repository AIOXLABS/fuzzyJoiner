\section{Triplet Selection}
Deep metric learning for faces is a difficult problem for neural networks to learn because improper sample selection can make it difficult for the network to converge.  We describe a popular approach from the face recognition literature first before we describe a new mechanism for triplet selection.

\subsection{Batch Based Triple Selection}
\begin{figure}
\includegraphics[width=1.0\linewidth]{triplet_selection}
\caption{Batch based triplet selection}
\label{triplet_selection}
\end{figure}

In Schroff et al.'s work~\cite{DBLP:conf/cvpr/SchroffKP15} they discuss a sample or mini-batch based triplet selection mechanism for training.  Conceptually, sampling the `right' triplets for fast network learning, as Schroff et al. point out requires sampling a set of \textit{hard positives} and a set of \textit{hard negatives}, where a hard positive is defined as $argmax^i \| x_{a} - x_{p}^i \|^2$, where $i$ ranges over all $x_p$ and a hard negative is defined as $argmin^i \| x_{a} - x{_n}^i \|^2$, where $i$ ranges over all $x_n$. 

However, it is infeasible to compute these values for the entire dataset.  Calculating hard positives is easy because the number of \textit{positives} is small normally.  Calculating hard negatives is not possible for all but small datasets.  As a result, the triplets can be generated either offline, for every n steps, or online, by selecting positive and negative exemplars within a batch of examples.  In Schroff et al.'s work, they focus on the online generation case.  

Figure~\ref{triplet_selection} shows the triplet selection mechanism used in their work.  In practice, instead of focusing on finding \textit{hard positives}, they instead pair every possible positive in the sample shown in the right panel in the figure with selected negatives, since the set of positives is usually quite small.  Furthermore, for negative examples, they try to select what they term \textit{semi-hard negatives} instead of \textit{hard negatives}, where a \textit{semi-hard negative} has the property that $\|x_a - x_p \|^2 < \|x_a - x_n \|^2$.  As shown in the figure~\ref{triplet_selection} a \textit{semi-hard negative} is further from the \textit{anchor} than the \textit{positive}, but is not distant enough.  That is, the loss function still needs to move it closer to the margin $\alpha$ that is used in the triplet-loss function.  Schroff et al. further argue selecting \textit{hard negatives} such as $C$ in the figure may in fact lead to bad local minima early on in training, and lead to a collapsed model where the function always learns to return a value of 0, regardless of the pair being considered.

\subsection{Metric Based Triplet Selection}
Although the computation of there is $argmax^i \| x_{a} - x_{p}^i \|^2$ and $argmin^i \| x_{a} - x{_n}^i \|^2$ is infeasible because it involves quadratic comparisons within a dataset, we observe that the use of approximate nearest neighbor (ANN) indexes can actually make this problem tractable for the selection of triples.  

Approximate nearest neighbor indexes are highly efficient methods for selecting the top-K neighbors of a given vector by Euclidean distance, cosine similarity or other distance metrics.  They are based on space partitioning algorithms, such as \textit{k-d trees}, where the vector space is iteratively bisected into two disjoint regions.  Finding the nearest neighbors of a vector involves traversal of the tree to a leaf by evaluating the queried vector at each split point in the tree.  Depending on the position of the vector within the space bounded by a split point, it may need to traverse both points at a split node.  However, large portions of the tree (and hence most vectors in the dataset) never get compared to the queried vector.  The average complexity to query the vectors of a neighbor is $O(log N)$ where N is the number of vectors in the dataset.  Implementations exist now for fast, memory-efficient ANN indexes that scale up to a billion vectors \cite{JDH17} using techniques to compress vectors in memory efficiently.  In our work, we used the Annoy ANN implementation \cite{annoy_git} in our work which is based on the refinements to \textit{k-d trees} ideas described in \cite{ann_paper}.

\begin{figure}
\includegraphics[width=1.0\linewidth]{ANN_selection}
\caption{ANN based triplet selection}
\label{ANN_selection}
\end{figure}

Assuming one has the entire dataset indexed in an ANN index, the problem of triplet selection can be simplified by asking the ANN index for the top k-nearest neighbors of an \textit{anchor}, where k is given by the number of triplets that one desires to generate for each anchor.  As in earlier work, selection of \textit{hard positives} is not really relevant because all positive data should be used to teach the network the right function.  Selection of negatives is the set of all nearest neighbors that are \textit{negatives}.  There is no explicit attempt to filter out\textit{hard negatives} in the approach.  The overall idea is that the set of negatives that appear in the nearest neighbor set at input are in fact the most important elements for the network to learn to discriminate from positives.  Focusing on these elements, regardless of whether they are \textit{hard} or \textit{semi-hard} should lead to better learning.

An ANN-based strategy works better than the batch based triplet selection approach on two counts.  First, the entire dataset is in fact used in the triplet selection process.  Second, it yields an important baseline to assess what if any learning was performed by the neural network in mapping input vectors to a different space.
