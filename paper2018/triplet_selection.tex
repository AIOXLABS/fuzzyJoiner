\subsection{Triplet Selection}
Deep metric learning for faces is a difficult problem for neural networks to learn because improper sample selection can make it difficult for the network to converge.  We describe a popular approach from the face recognition literature first before we describe a new mechanism for triplet selection.

\subsubsection{Batch Based Triple Selection}
In Schroff et al.'s work~\cite{DBLP:conf/cvpr/SchroffKP15} they discuss various online and offline approaches for the problem of triplet selection.  Conceptually, sampling the `right' triples for fast network learning, as Schroff et al. point out requires sampling a set of \textit{hard positives} and a set of \textit{hard negatives}, where a hard positive is defined as $argmax^i \| x_{a} - x_{p}^i \|^2$, where $i$ ranges over all $x_p$ and a hard negative is defined as $argmin^i \| x_{a} - x{_n}^i \|^2$.  

However, it is infeasible to compute these values for the entire dataset for selection.  As a result, the triplets can be generated either offline, for every n steps, or online, by selecting the hard positive and negative exemplars within a batch of examples.  In Schroff et al.'s work, they focus on the online generation case.  

In practice, instead of focusing on finding \textit{hard positives}, they instead pair every possible positive in the sample with selected negatives, since the set of positives is usually quite small.  Furthermore, for negative examples, they try to select what they term \textit{semi-hard negatives} instead of \textit{hard negatives}, where a \textit{semi-hard negative} has the property that $\|x_a - x_p \|^2 < \|x_a - x_n \|^2$.  That is, \textit{semi-hard negative} is further from the \textit{anchor} than the \textit{positive}, but is not distant by the margin $\alpha$ that is used in the triplet-loss function.  Schroff et al. argue selecting \textit{hard negatives} may in practice lead to bad local minima early on in training, and lead to a collapsed model where the function always learns to return a value of 0, regardless of the pair being considered.

\subsubsection{Metric Based Triplet Selection}
Although the computation of there is $argmax^i \| x_{a} - x_{p}^i \|^2$ and $argmin^i \| x_{a} - x{_n}^i \|^2$ is infeasible because it involves quadratic comparisons within a dataset, we observe that the use of approximate nearest neighbor (ANN) indexes can actually make this problem tractable for the selection of triples.  

Approximate nearest neighbor indexes are highly efficient methods for selecting the top-K neighbors of a given vector by Euclidean distance or cosine similarity.  They are based on space partitioning algorithms, such as \textit{k-d trees}, where the vector space is iteratively bisected into two disjoint regions.  Finding the nearest neighbors of a vector involves traversal of the tree to a leaf by evaluating the queried vector at each split point in the tree.  Depending on the position of the vector within the space bounded by a split point, it may need to traverse both points the split.  However, large portions of the tree (and hence most vectors in the dataset) never get compared to each other.  The average complexity to query the vectors of a neighbor is in fact $O(log N)$ where N is the number of vectors in the dataset.  Implementations exist now for fast, memory-efficient ANN indexes that scale up to a billion vectors \cite{JDH17} using techniques to compress vectors in memory efficiently.  In our work, we used the Annoy ANN implementation \cite{annoy_git} in our work which is based on the refinements to \textit{k-d trees} ideas described in \cite{annoy-paper}.

Assuming one has the entire dataset indexed in an ANN index, the problem of triplet selection can be simplified by asking the ANN index for the top k-nearest neighbors of an \textit{anchor}, where k is given by the number of triplets that one desires to generate for each anchor.  Selection of \textit{hard positives} is not really relevant because all positive data should be used to teach the network the right function, as Schroff et al. point out.  \textit{Hard negatives} are simply the negatives that appear in the top-K.  

The selection of \textit{semi-hard negatives} is a bit more difficult and depends on the characteristics of the dataset.  In some cases, there may be very few \textit{semi-hard negatives} because the positives are more distant than the negatives.  This was a rather serious problem in our people's name matching dataset.  If we take the character embeddings of each of the names in our name matching dataset, index it in an ANN, and then ask for the top-k neighbors we find somewhat troubling properties for an approach that tries to select \textit{semi-hard} negatives.  First, \textit{positives} appeared in the top 20 neighbors only in 3.2\% of the dataset.  Explicitly computing the distance between the \textit{anchor} and the \textit{positive} yielded a mean of 9.07, with a standard deviation of 3.07.  The mean distances between the \textit{anchor} and the \textit{negatives} for those negatives in the top 20 was 3.16 with a standard deviation of 1.17.  Given that only 3.2\% of positives actually even appeared in the top-K nearest neighbors, selection of \textit{semi-hard} negatives is reduced to a very small fraction of the dataset.  We therefore used a k-nearest neighbor approach to triplet selection instead, picking all positive triplets, and a set of \textit{hard negative} triplets based on the top-k nearest neighbors.  As we show later this approach generalizes to datasets such as company names where it is less extreme in terms of reversal of \textit{positive} and \textit{negative} distances.
