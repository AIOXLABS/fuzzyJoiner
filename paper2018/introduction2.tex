\section{Introduction}

Merging datasets is a key operation for data analytics. A frequent requirement for merging is a join between two columns that frequently have different surface representations for the same entity (e.g., the name of a person might be represented as \textit{Douglas Adams} or \textit{Douglas Noel Adams}, as shown in Table~\ref{table-example}.  This problem occurs for many entity types such as people's names, company names, addresses, product descriptions, conference venues, or even people's faces.  Data management systems have however, largely focussed solely on equi-joins, where string or numeric equality determines which rows should be joined, because such joins are efficient.

Extensions in data management systems for handling `fuzzy' joins across different surface forms for the same entity typically use string similarity algorithms such as edit-distance, Jaro-Winkler and TF-IDF (e.g., \cite{Cohen2003}).  In applying fuzzy joins, a key problem is to avoid quadratic comparisons between pairs of strings drawn from the two columns.  A blocking strategy is used to reduce the number of pairs for fuzzy joins, such that only strings that share a common prefix, or suffix will be compared further.  String matching algorithms often do not work however, because transformations of entity names in perfectly valid ways can yield very different strings.  As shown in Table~\ref{table-example}, \textit{John Smith} is more similar to \textit{John Adams} than \textit{Adams, John}, but of course it is \textit{Adams, John} that is the valid alternate surface form for \textit{John Adams}.

More recently, data driven approaches have emerged as a powerful alternative to string matching techniques for fuzzy joins.  Data driven approaches mine patterns in the data to determine the `rules' for joining a given entity type.  One example of such an approach is illustrated in \cite{He:2015:SJS:2824032.2824036}, which determines which cell values should be joined based on whether those cell values co-occur on the same row across disparate tables in a very large corpus of data.  Another example of a data driven approach is work by \cite{auto-join-joining-tables-leveraging-transformations} where program synthesis techniques are used to learn the right set of transformations needed to perform the entity matching operation.  

In this paper, we propose a novel approach to the problem of joining different surface representations of the same entity, inspired in part by recent advances in deep learning.  Our approach depends on (a) creating a function that maps surface forms into a set of vectors such that alternate forms for the same entity are closest in vector space, (b) indexing these vectors using a space partitioning algorithm to find the forms that can be potentially joined together.  Our approach for merging datasets is much more flexible than prior data driven approaches (e.g., \cite{He:2015:SJS:2824032.2824036}, \cite{auto-join-joining-tables-leveraging-transformations}) because it can be applied to any data provided one can create a mapping function such that joinable surface forms of the entity are closer in vector space than disparate forms.  For instance, in face recognition and person re-identification, there has been a significant research effort aimed at creating deep learning models that learn to map different surface forms of a face closer together in a vector space.  In our proposed approach, we can now join two columns if they have alternate faces for the same entity, whereas in prior work, those faces needed to either have been joined before in some existing dataset, or the transforms of a face need to be discoverable as a program, which is near impossible for faces.  Our approach for merging datasets is also efficient because it eliminates quadratic comparisons of cell values in each column by using a mechanism that is itself data-driven.  Because we use space partitioning algorithms to find surface forms that are potentially joinable, the search process is efficiently eliminates large parts of the vector space from consideration for any given queried vector.  Across all cell values that are considered for a join, the efficiency of the search is $\mathcal{O}(n\log{}n)$.

Specifically, the first part of our solution to the join problem involves building a deep learning model that performs metric learning.  For the closely related problem of face recognition and person re-identification, deep neural networks have been used successfully to build models that map faces of the same entity closer in vector space estimate relative to faces from other entities.  To test the generality of our approach, we tried to build a deep learning model model for names of people and companies. As in face recognition, our system for metric learning is built by training a `triplet siamese network' to learn to map different surface forms of the same entity closer in vector space than different forms of the same entity.  Entity names have very different properties from faces, so we needed to adapt the approaches used in face recognition to the problem of matching entity names with a novel method to choose the sample for training, and novel loss functions as objectives for the metric learning problem.  At training, the network is given a triplet consisting of an `anchor' (e.g. \textit{Douglas Adams}), a positive element (e.g. \textit{Douglas Noel Adams}) and a negative element (e.g., John Adams), and it learns to produce a small distance estimate for the positive pair (close to 0), and a large distance estimate for the negative pair, using an objective function. A classification form of this function that simply learns whether two names belong to the same entity can be used to perform fuzzy joins, but then we would need a blocking strategy analogous to what has been used in the literature to identify the set of joinable pairs \cite{auto-join-joining-tables-leveraging-transformations}.  

Our observation is that one can actually exploit what the siamese network produces to eliminate this blocking step altogether.  Specifically, the last hidden layer of the siamese network is effectively a `vector embedding' that contains the critical features needed for computing the distance estimate.  If such models are pre-built for a given entity type, we can then use the models to produce vector embeddings for all cell values in the two columns to be joined in a single linear pass, and index just the second column values with a space partitioning algorithm that lays out vectors in n-dimensional space.  Then, for each cell value in the first column to be joined, querying the k-nearest neighbors should provide the set of values in the second column that can be joined. In our work, we used approximate nearest neighbor algorithms, which have been applied successfully to billions of items \cite{JDH17}.

\begin{table}[!htb]
    \caption{Example of a merge problem}
    \begin{subtable}{.5\linewidth}
      \centering
        \caption{Table A}
        \begin{tabular}{|l|l|}
          \hline
           Dept & Name \\
           \hline
           1    & Douglas Adams \\
           2    & John Adams \\
           3  & Douglas Bard \\
           \hline
        \end{tabular}
    \end{subtable}%
    \begin{subtable}{.5\linewidth}
      \centering
        \caption{Table B}
        \begin{tabular}{|l|l|}
          \hline
           Name & Salary \\
           \hline
           Douglas Noel Adams & 2000 \\
           Adams, John & 3000 \\
           John Smith & 3000 \\
           \hline
        \end{tabular}
    \end{subtable}
    \label{table-example}
\end{table}

Our key contributions are as follows:
\begin{itemize}
\item We outline a general approach to the problem of merging datasets for different surface forms of the same entity.  Our approach consists of building a deep neural network model for specific entity types and using a space partitioning algorithm to find the nearest neighbors to join.  To test the generality of our approach, we built triplet loss based siamese networks for ~200,000 people's names and ~70,000 company names respectively, drawn from Wikidata.  We show that such networks did learn to map surface forms of the same entity closer in vector space.  Specifically, if we 83\% and 79\% of variants of the entity name appeared in the top 20 neighbors for each entity.  Moreover, within the 20 neighbors, 77\% and 75\% of the positive names were closer to the anchor than negative names for people and companies respectively.  Our results suggest that deep learning models can be used successfully for data driven joins, assuming that such models are built for common entity types.
\item Training neural networks for deep metric learning is difficult because one cannot show quadratic combinations of the data to the system.  In face recognition, a number of techniques exist for triplet selection, and methods for selecting triples is an active area of research.  We developed a new technique for triplet selection for entity names because the techniques from face recognition do not generalize well to entity names.  We also developed a novel loss function to train the network to discriminate entity names from names of other entities.  We compare the results for this loss function with the ones that have been used for face recognition, and show that this loss function can perform much better than the existing functions for deep learning in this domain.
\item We also perform a complexity analysis of how such models can then be used to perform joins across different surface forms of the same entity efficiently, with a complexity of $\mathcal{O}(n\log{}n)$.  
\end{itemize} 

The rest of the paper is organized as follows: Section~\ref{siamese networks} describes how we adapted siamese networks to the problem of entity matching, specifically the problem of triplet selection for entity names, and the need for a new type of loss function to improve metric learning for this problem, section ~\ref{joins} describes how to use these models for efficient joins across datasets, section ~\ref{dataset} describes characteristics of the datasets we used for training, as well as cleansing and data augmentation techniques we used to generate data, section ~\ref{results} provides the experimental results, section~\ref{related work} covers related work, and section~\ref{conclusions} describes future work and conclusions.
