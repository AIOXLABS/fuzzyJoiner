\section{Introduction}

Merging datasets is a key operation for data analytics. A frequent requirement for merging is a join between two columns that frequently have different surface representations for the same entity (e.g., the name of a person might be represented as \textit{Douglas Adams} or \textit{Douglas Noel Adams}, as shown in Table~\ref{table-example}.  This problem occurs for many entity types such as people's names, company names, addresses, product descriptions, conference venues, or even people's faces.  Data management systems have however, largely focussed solely on equi-joins, where string or numeric equality determines which rows should be joined, because such joins are efficient.

Extensions in data management systems for handling `fuzzy' joins across different surface forms for the same entity typically use string similarity algorithms such as edit-distance, Jaro-Winkler and TF-IDF (e.g., \cite{Cohen2003}).  In applying fuzzy joins, a key problem is to avoid quadratic comparisons between pairs of strings drawn from the two columns.  A blocking strategy is used to reduce the number of pairs for fuzzy joins, such that only strings that share a common prefix, or suffix will be compared further.  String matching algorithms often do not work however, because transformations of entity names in perfectly valid ways can yield very different strings.  As shown in Table~\ref{table-example}, \textit{John Smith} is more similar to \textit{John Adams} than \textit{Adams, John}, but of course it is \textit{Adams, John} that is the valid alternate surface form for \textit{John Adams}.

More recently, data driven approaches have emerged as a powerful alternative to string matching techniques for fuzzy joins.  Data driven approaches mine patterns in the data to determine the `rules' for joining a given entity type.  One example of such an approach is illustrated in \cite{He:2015:SJS:2824032.2824036}, which determines which cell values should be joined based on whether those cell values co-occur on the same row across disparate tables in a very large corpus of data.  Another example of a data driven approach is work by \cite{auto-join-joining-tables-leveraging-transformations} where program synthesis techniques are used to learn the right set of transformations needed to perform the entity matching operation.  

In this paper, we propose a different approach to the problem of joining different surface representations of the same entity, inspired in part by recent advances in deep learning.  Our approach depends on (a) creating a function that maps surface forms into a set of vectors such that alternate forms for the same entity are closest in vector space, (b) indexing these vectors using a space partitioning algorithm to find the forms that can be potentially joined together.  Our approach for merging datasets is much more flexible than prior data driven approaches (e.g., \cite{He:2015:SJS:2824032.2824036}, \cite{auto-join-joining-tables-leveraging-transformations}) because it can be applied to any data provided one can create a mapping function such that joinable surface forms of the entity are closer in vector space than disparate forms.  For instance, in face recognition and person re-identification, there has been a significant research effort aimed at creating deep learning models that learn to map different surface forms of a face closer together in a vector space (this technique is often referred to as deep metric learning).  In our proposed approach, we can now join two columns if they have alternate faces for the same entity, whereas in prior work, those faces needed to either have been joined before in some existing dataset, or the transforms of a face need to be discoverable as a program, which is near impossible for faces.  Our approach for merging datasets is also efficient because it eliminates quadratic comparisons of cell values in each column by using a mechanism that is itself data-driven.  Because we use space partitioning algorithms (such as approximate nearest neighbor) to find surface forms that are potentially joinable, the search process is efficiently eliminates large parts of the vector space from consideration for any given queried vector.  Across all cell values that are considered for a join, the efficiency of the search is $\mathcal{O}(n\log{}n)$.  Implementations exist for nearest neighbor algorithms that have been applied to billions of vectors \cite{JDH17}, so the approach is practical for most datasets.  

The idea of building joint embeddings for merging followed by nearest neighbors search has been applied recently to a slightly different problem of linking relational tuple embeddings with embeddings of other relational tuples or unstructured text \cite{Bordawekar18}, \cite{IDEL18}).  While our approach is conceptually similar to these two pieces of work, our contribution is that we developed general purpose embedding models for merging alternate surface forms of key entities such as people and companies leveraged from linked open data. Using Wikidata as our ground truth, we built models for datasets with ~1.1M people's names (~200K identities) and 130K company names (~70K identities).  Deep metric learning is known to be a difficult problem in the closely related space of face recognition and person-re-identification \cite{DBLP:conf/cvpr/SchroffKP15}.  To build these models for people and companies, we developed a novel approach for selecting samples for training, as well as developed a novel loss function, as we describe below.

As in face recognition, our system for metric learning is built by training a `triplet siamese network' to learn to map different surface forms of the same entity closer in vector space than different forms of the same entity.  At training, the network is given a triplet consisting of an `anchor' (e.g. \textit{Douglas Adams}), a positive element (e.g. \textit{Douglas Noel Adams}) and a negative element (e.g., John Adams), and it learns to produce a small distance estimate for the positive pair (close to 0), and a large distance estimate for the negative pair, using an objective function. The last hidden layer of the siamese network is effectively a `vector embedding' that contains the critical features needed for computing the distance estimate.  There has been a significant amount of research in the problem of triplet selection, starting with the original Facenet work \cite{DBLP:conf/cvpr/SchroffKP15}.  For any given training set, the key question is how to choose the set of negatives to teach the system the right mapping quickly.  In prior work, for instance, various triplet mining techniques have been used to estimate which negatives might be good to pair with positives for training.  As one example, \cite{DBLP:conf/cvpr/SchroffKP15} perform pairwise comparisons within a batch of training examples to identify \textit{semi-hard negatives} which are negatives that are slightly further away from the anchor than the positive example, but not distant enough from the anchor by a sufficient margin.  One problem with this approach is that it relies on the existence of very large training sets and batches, so that one can find a sufficient number of negative examples for training in a batch.  Our observation is that we can re-use the approximate nearest neighbors approach for triplet selection as well.  Because our ultimate goal is to train the system to learn to map positives closer to the anchor and negatives further away from the anchor, we could select negatives that appear in the nearest neighbor set for input vectors, before any training has taken place.  Depending on the nature of the dataset, these negatives could well be \textit{hard negatives}, i.e., negatives that are closer to the anchor than the positives.  In fact, in our work, most of the triplets we selected had \textit{hard negatives}.  \cite{DBLP:conf/cvpr/SchroffKP15} suggest that using only \textit{hard negatives} might be problematic for metric learning, but in our work, we used this approach successfully to train two separate models for people and companies respectively.  Our observation is that triplet selection based on a nearest neighbors algorithm is a principled and efficient mechanism for selecting triples to train deep metric learning models.

There have also been a number of different objective functions that have been proposed to build better models.  For the problem of building models for people's names, we found that a modification to the objective function proposed in \cite{DBLP:conf/cvpr/SchroffKP15} was better than three other functions that have been proposed in the literature. We show the performance of the different functions for the two datasets we considered.

\begin{table}[!htb]
    \caption{Example of a merge problem}
    \begin{subtable}{.5\linewidth}
      \centering
        \caption{Table A}
        \begin{tabular}{|l|l|}
          \hline
           Dept & Name \\
           \hline
           1    & Douglas Adams \\
           2    & John Adams \\
           3  & Douglas Bard \\
           \hline
        \end{tabular}
    \end{subtable}%
    \begin{subtable}{.5\linewidth}
      \centering
        \caption{Table B}
        \begin{tabular}{|l|l|}
          \hline
           Name & Salary \\
           \hline
           Douglas Noel Adams & 2000 \\
           Adams, John & 3000 \\
           John Smith & 3000 \\
           \hline
        \end{tabular}
    \end{subtable}
    \label{table-example}
\end{table}

Our key contributions are as follows:
\begin{itemize}
\item We outline a general approach to the problem of merging datasets for different surface forms of the same entity.  Our approach consists of building a deep neural network model for specific entity types and using a space partitioning algorithm to find the nearest neighbors to join.  To test the generality of our approach, we built triplet loss based siamese networks for 200K people's names and 70K company names respectively, drawn from Wikidata.  We show that such networks did learn to map surface forms of the same entity closer in vector space, with a recall rate of 81\% and 75\% after training for 20 neighbors.  Moreover, within the 20 neighbors, 93\% and 96\% of the positive names were closer to the anchor than negative names for people and companies respectively, suggesting good precision.  Our results suggest that deep learning models can be used successfully for data driven joins, assuming that such models are built for common entity types.
\item Training neural networks for deep metric learning is difficult because one cannot show quadratic combinations of the data to the system.  In face recognition, a number of techniques exist for triplet selection, and methods for selecting triples is an active area of research.  We developed a new technique for triplet selection for entity names based on an approximate nearest neighbors algorithm over input vectors.  We also explored whether a modified loss function can be used to train the network more effectively, and found that it does in fact perform better for training on people's names.
\item A key element of our argument is that once these embedding models have been built, they can be applied to perform joins efficiently for a new dataset.  We test if this is true with previously built face recognition models, because of the paucity of large datasets for companies and people, and the wealth of datasets in the face recognition literature.  For the most part, deep metric models for faces have been used with a classification layer have been used to identify a specific individual, or for clustering faces to measure goodness of the clustering algorithm.  Because we use these existing models for a slightly different purpose, we calculated the same metrics of precision and recall that we calculated for people and companies. We compared the performance of the Facenet model that was trained on VGGFace2 \cite{DBLP:conf/fgr/CaoSXPZ18} on the Labeled Faces in the Wild dataset \cite{Huang_labeledfaces}.  Our results show a recall of 80\%, and a precision of 93\%.  
\end{itemize} 

The rest of the paper is organized as follows: Section~\ref{siamese networks} describes how we adapted siamese networks to the problem of entity matching, specifically the problem of triplet selection for entity names, and the need for a new type of loss function to improve metric learning for this problem, section ~\ref{joins} describes how to use these models for efficient joins across datasets, section ~\ref{dataset} describes characteristics of the datasets we used for training, as well as cleansing and data augmentation techniques we used to generate data, section ~\ref{results} provides the experimental results, section~\ref{related work} covers related work, and section~\ref{conclusions} describes future work and conclusions.
